{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and encode the data into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded features shape: (118917, 175)\n",
      "\n",
      "Feature names: ['che_pc_usd', 'che_perc_gdp', 'insurance_perc_che', 'population', 'prev_perc', 'price_month', 'price_unit', 'public_perc_che', 'target', 'months_since_launch', 'launch_date_year', 'launch_date_month', 'date_year', 'date_month', 'brand_encoded', 'cluster_nl_encoded', 'corporation_encoded', 'country_encoded', 'drug_id_encoded', 'therapeutic_area_encoded', 'indication_0', 'indication_1', 'indication_2', 'indication_3', 'indication_4', 'indication_5', 'indication_6', 'indication_7', 'indication_8', 'indication_9', 'indication_10', 'indication_11', 'indication_12', 'indication_13', 'indication_14', 'indication_15', 'indication_16', 'indication_17', 'indication_18', 'indication_19', 'indication_20', 'indication_21', 'indication_22', 'indication_23', 'indication_24', 'indication_25', 'indication_26', 'indication_27', 'indication_28', 'indication_29', 'indication_30', 'indication_31', 'indication_32', 'indication_33', 'indication_34', 'indication_35', 'indication_36', 'indication_37', 'indication_38', 'indication_39', 'indication_40', 'indication_41', 'indication_42', 'indication_43', 'indication_44', 'indication_45', 'indication_46', 'indication_47', 'indication_48', 'indication_49', 'indication_50', 'indication_51', 'indication_52', 'indication_53', 'indication_54', 'indication_55', 'indication_56', 'indication_57', 'indication_58', 'indication_59', 'indication_60', 'indication_61', 'indication_62', 'indication_63', 'indication_64', 'indication_65', 'indication_66', 'indication_67', 'indication_68', 'indication_69', 'indication_70', 'indication_71', 'indication_72', 'indication_73', 'indication_74', 'indication_75', 'indication_76', 'indication_77', 'indication_78', 'indication_79', 'indication_80', 'indication_81', 'indication_82', 'indication_83', 'indication_84', 'indication_85', 'indication_86', 'indication_87', 'indication_88', 'indication_89', 'indication_90', 'indication_91', 'indication_92', 'indication_93', 'indication_94', 'indication_95', 'indication_96', 'indication_97', 'indication_98', 'indication_99', 'indication_100', 'indication_101', 'indication_102', 'indication_103', 'indication_104', 'indication_105', 'indication_106', 'indication_107', 'indication_108', 'indication_109', 'indication_110', 'indication_111', 'indication_112', 'indication_113', 'indication_114', 'indication_115', 'indication_116', 'indication_117', 'indication_118', 'indication_119', 'indication_120', 'indication_121', 'indication_122', 'indication_123', 'indication_124', 'indication_125', 'indication_126', 'indication_127', 'indication_128', 'indication_129', 'indication_130', 'indication_131', 'indication_132', 'indication_133', 'indication_134', 'indication_135', 'indication_136', 'indication_137', 'indication_138', 'indication_139', 'indication_140', 'indication_141', 'indication_142', 'indication_143', 'indication_144', 'indication_145', 'indication_146', 'indication_147', 'indication_148', 'indication_149', 'indication_150', 'indication_151', 'indication_152', 'indication_153', 'indication_154']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset (replace 'your_dataset.csv' with your file path)\n",
    "df = pd.read_csv('../data/intermig/train_data-medianimputed.csv')\n",
    "df = df.drop('ind_launch_date', axis=1)\n",
    "\n",
    "\n",
    "date_cols = ['launch_date', 'date']\n",
    "\n",
    "# Process date columns\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "    df[f'{col}_year'] = df[col].dt.year\n",
    "    df[f'{col}_month'] = df[col].dt.month\n",
    "    df = df.drop(col, axis=1)\n",
    "\n",
    "categoric_cols = ['brand', 'cluster_nl', 'corporation', 'country', 'drug_id', 'therapeutic_area']\n",
    "\n",
    "# Process categorical columns\n",
    "label_encoders = {}\n",
    "for col in categoric_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[f'{col}_encoded'] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "    df = df.drop(col, axis=1)\n",
    "\n",
    "# Handle indication column (contains lists)\n",
    "mlb = MultiLabelBinarizer()\n",
    "# Convert string representations of lists to actual lists\n",
    "df['indication'] = df['indication'].apply(ast.literal_eval)\n",
    "# Transform the lists\n",
    "indication_encoded = pd.DataFrame(\n",
    "    mlb.fit_transform(df['indication']),\n",
    "    columns=[f'indication_{i}' for i in range(len(mlb.classes_))],\n",
    "    index=df.index\n",
    ")\n",
    "df = df.drop('indication', axis=1)\n",
    "df = pd.concat([df, indication_encoded], axis=1)\n",
    "\n",
    "print(\"Encoded features shape:\", df.shape)\n",
    "print(\"\\nFeature names:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the graph with dataset rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# 1. Organize node features\n",
    "numerical_features = [\n",
    "    'che_pc_usd', 'che_perc_gdp', 'insurance_perc_che', 'population',\n",
    "    'prev_perc', 'price_month', 'price_unit', 'public_perc_che',\n",
    "    'months_since_launch'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'brand_encoded', 'cluster_nl_encoded', 'corporation_encoded',\n",
    "    'country_encoded', 'drug_id_encoded', 'therapeutic_area_encoded'\n",
    "]\n",
    "\n",
    "indication_features = [col for col in df.columns if col.startswith('indication_')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Construct graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Group by cluster_nl to get time series\n",
    "cluster_features = {}\n",
    "for cluster_nl, group in df.groupby('cluster_nl_encoded'):\n",
    "    # Sort by date\n",
    "    group = group.sort_values('date_year')\n",
    "    \n",
    "    # Get static features (last values)\n",
    "    static_features = np.concatenate([\n",
    "        group[numerical_features].iloc[-1].values,\n",
    "        group[categorical_features].iloc[-1].values,\n",
    "        group[indication_features].iloc[-1].values\n",
    "    ])\n",
    "    \n",
    "    # Add temporal features\n",
    "    temporal_features = np.array([\n",
    "        group['date_year'].min(),     # launch_year\n",
    "        group['date_month'].min(),    # launch_month\n",
    "        len(group),                   # duration\n",
    "        group['target'].mean(),       # mean_target\n",
    "    ])\n",
    "\n",
    "    # Combine features for similarity computation\n",
    "    cluster_features[cluster_nl] = np.concatenate([\n",
    "        group[numerical_features].iloc[-1].values,\n",
    "    ])\n",
    "        \n",
    "    # Add node\n",
    "    G.add_node(cluster_nl,\n",
    "                features=static_features,\n",
    "                temporal=temporal_features,\n",
    "                country=group['country_encoded'].iloc[0],\n",
    "                brand=group['brand_encoded'].iloc[0],\n",
    "                corporation=group['corporation_encoded'].iloc[0],\n",
    "                therapeutic_area=group['therapeutic_area_encoded'].iloc[0],\n",
    "                ts_data=group['target'].values)  # Store full time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph Statistics:\n",
      "Number of nodes: 2716\n",
      "Number of edges: 1026822\n",
      "\n",
      "Edge types distribution:\n",
      "therapeutic: 721865 edges\n",
      "country: 103988 edges\n",
      "corporation: 200730 edges\n",
      "similarity: 239 edges\n"
     ]
    }
   ],
   "source": [
    "# Add edges between similar time series\n",
    "clusters = list(G.nodes())\n",
    "feature_matrix = np.vstack([cluster_features[c] for c in clusters])\n",
    "similarity_matrix = cosine_similarity(feature_matrix)\n",
    "similarity_matrix\n",
    "\n",
    "similarity_threshold=0.99\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    for j in range(i+1, len(clusters)):\n",
    "        ci, cj = clusters[i], clusters[j]      \n",
    "\n",
    "        consider_similarity = True \n",
    "        # Connect if same therapeutic area\n",
    "        if G.nodes[ci]['therapeutic_area'] == G.nodes[cj]['therapeutic_area']:\n",
    "            G.add_edge(ci, cj, edge_type='therapeutic')\n",
    "            consider_similarity = False \n",
    "\n",
    "        # Connect if same corporation\n",
    "        if G.nodes[ci]['corporation'] == G.nodes[cj]['corporation']:\n",
    "            G.add_edge(ci, cj, edge_type='corporation')\n",
    "            consider_similarity = False \n",
    "\n",
    "        # Connect if same country\n",
    "        if G.nodes[ci]['country'] == G.nodes[cj]['country']:\n",
    "            G.add_edge(ci, cj, edge_type='country')\n",
    "            consider_similarity = False \n",
    "\n",
    "        if not consider_similarity:\n",
    "            continue\n",
    "       # Connect if similar features\n",
    "        if similarity_matrix[i, j] > similarity_threshold:\n",
    "            G.add_edge(ci, cj, edge_type='similarity')\n",
    "        \n",
    "# Print graph statistics\n",
    "print(f\"\\nGraph Statistics:\")\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(\"\\nEdge types distribution:\")\n",
    "edge_types = [d['edge_type'] for (u, v, d) in G.edges(data=True)]\n",
    "for edge_type in set(edge_types):\n",
    "    count = edge_types.count(edge_type)\n",
    "    print(f\"{edge_type}: {count} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Data Statistics:\n",
      "Number of node features: 170\n",
      "Number of edges: 2053644\n",
      "Number of edge types: 4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Convert graph to PyTorch Geometric format\n",
    "def convert_to_pytorch_geometric(G):\n",
    "    # Prepare edge index and type\n",
    "    edge_index = []\n",
    "    edge_type = []\n",
    "    edge_type_dict = {\n",
    "        'therapeutic': 0,\n",
    "        'corporation': 1,\n",
    "        'country': 2,\n",
    "        'similarity': 3\n",
    "    }\n",
    "    \n",
    "    for u, v, data in G.edges(data=True):\n",
    "        # Add both directions for undirected graph\n",
    "        edge_index.extend([[u, v], [v, u]])\n",
    "        edge_type.extend([edge_type_dict[data['edge_type']]] * 2)\n",
    "    \n",
    "    # Convert node features\n",
    "    x = torch.tensor([G.nodes[node]['features'] for node in G.nodes()], dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "    edge_type = torch.tensor(edge_type, dtype=torch.long)\n",
    "    \n",
    "    # Create target tensor\n",
    "    y = torch.tensor([G.nodes[node]['ts_data'][-1] for node in G.nodes()], dtype=torch.float)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, edge_type=edge_type, y=y)\n",
    "\n",
    "# Convert graph and initialize model\n",
    "data = convert_to_pytorch_geometric(G)\n",
    "\n",
    "# Move data to device\n",
    "data = data.to(device)\n",
    "\n",
    "\n",
    "print(f\"\\nData Statistics:\")\n",
    "print(f\"Number of node features: {data.x.size(1)}\")\n",
    "print(f\"Number of edges: {data.edge_index.size(1)}\")\n",
    "print(f\"Number of edge types: {len(torch.unique(data.edge_type))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "TimeSeriesGNN(\n",
      "  (spatial_layers): ModuleList(\n",
      "    (0-3): 4 x GATConv(170, 8, heads=4)\n",
      "  )\n",
      "  (temporal_conv): Sequential(\n",
      "    (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      "  (temporal_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "\n",
    "class TimeSeriesGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, forecast_steps=23, num_edge_types=4):\n",
    "        super().__init__()\n",
    "        self.num_edge_types = num_edge_types\n",
    "        \n",
    "        # Edge type attention weights\n",
    "        self.edge_type_weights = nn.Parameter(torch.ones(num_edge_types))\n",
    "        \n",
    "        # Spatial: GAT layers per edge type\n",
    "        self.spatial_layers = nn.ModuleList([\n",
    "            GATConv(in_channels, hidden_channels, heads=4, concat=False) \n",
    "            for _ in range(num_edge_types)\n",
    "        ])\n",
    "        \n",
    "        # Temporal: Transformer + Conv with normalization (good suggestion!)\n",
    "        self.temporal_conv = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.temporal_attention = nn.MultiheadAttention(hidden_channels, num_heads=4)\n",
    "        \n",
    "        # Output layers with regularization (good suggestion!)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_channels * 2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_channels, forecast_steps * 2)\n",
    "        )\n",
    "\n",
    "    def nll_loss(self, means, log_vars, targets):\n",
    "        variances = torch.exp(log_vars)\n",
    "        loss = ((targets - means)**2 / variances + log_vars).mean()\n",
    "        return loss\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_type, batch=None):\n",
    "        # 1. Spatial dependencies\n",
    "        spatial_embeds = []\n",
    "        for i in range(self.num_edge_types):\n",
    "            mask = edge_type == i\n",
    "            if mask.any():\n",
    "                edge_index_i = edge_index[:, mask]\n",
    "                spatial_embed = self.spatial_layers[i](x, edge_index_i)\n",
    "                spatial_embeds.append(spatial_embed)\n",
    "            else:\n",
    "                spatial_embeds.append(torch.zeros_like(x))\n",
    "        \n",
    "        # Combine spatial embeddings\n",
    "        spatial_out = sum(spatial_embeds)\n",
    "        \n",
    "        # 2. Temporal processing\n",
    "        # Convolution branch\n",
    "        temp_conv = self.temporal_conv(spatial_out.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # Attention branch\n",
    "        temp_attn, _ = self.temporal_attention(\n",
    "            spatial_out.unsqueeze(0),\n",
    "            spatial_out.unsqueeze(0),\n",
    "            spatial_out.unsqueeze(0)\n",
    "        )\n",
    "        temp_attn = temp_attn.squeeze(0)\n",
    "        \n",
    "        # Combine temporal features\n",
    "        combined = torch.cat([temp_conv, temp_attn], dim=-1)\n",
    "        \n",
    "        # 3. Generate forecasts with uncertainty\n",
    "        output = self.decoder(combined)\n",
    "        means, log_vars = output.chunk(2, dim=-1)\n",
    "        \n",
    "        return means, torch.exp(log_vars)  # return mean and variance\n",
    "\n",
    "# Convert graph and initialize model\n",
    "model = TimeSeriesGNN(\n",
    "    in_channels=data.x.size(1),\n",
    "    hidden_channels=8,\n",
    "    forecast_steps=2,\n",
    "    num_edge_types=4\n",
    ").to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[64], expected input with shape [*, 64], but got input of size[8, 64, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 35\u001b[0m\n\u001b[0;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m TimeSeriesGNN(\n\u001b[0;32m     28\u001b[0m     in_channels\u001b[38;5;241m=\u001b[39min_channels,\n\u001b[0;32m     29\u001b[0m     hidden_channels\u001b[38;5;241m=\u001b[39mhidden_channels,\n\u001b[0;32m     30\u001b[0m     forecast_steps\u001b[38;5;241m=\u001b[39mforecast_steps,\n\u001b[0;32m     31\u001b[0m     num_edge_types\u001b[38;5;241m=\u001b[39mnum_edge_types\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m means, log_vars \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mnll_loss(means, log_vars, targets)\n",
      "File \u001b[1;32mc:\\dev\\Bonkis-Jake\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\dev\\Bonkis-Jake\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[103], line 57\u001b[0m, in \u001b[0;36mTimeSeriesGNN.forward\u001b[1;34m(self, x, edge_index, edge_type, batch)\u001b[0m\n\u001b[0;32m     53\u001b[0m spatial_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(spatial_embeds)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# 2. Temporal processing\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Convolution branch\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m temp_conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemporal_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspatial_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Attention branch\u001b[39;00m\n\u001b[0;32m     60\u001b[0m temp_attn, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_attention(\n\u001b[0;32m     61\u001b[0m     spatial_out\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     62\u001b[0m     spatial_out\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     63\u001b[0m     spatial_out\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     64\u001b[0m )\n",
      "File \u001b[1;32mc:\\dev\\Bonkis-Jake\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\dev\\Bonkis-Jake\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\dev\\Bonkis-Jake\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\dev\\Bonkis-Jake\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\dev\\Bonkis-Jake\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\dev\\Bonkis-Jake\\venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\dev\\Bonkis-Jake\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2900\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2892\u001b[0m         layer_norm,\n\u001b[0;32m   2893\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2898\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m   2899\u001b[0m     )\n\u001b[1;32m-> 2900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given normalized_shape=[64], expected input with shape [*, 64], but got input of size[8, 64, 1]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create sample data\n",
    "batch_size = 4\n",
    "in_channels = 10  # numerical + categorical + indication features\n",
    "hidden_channels = 64\n",
    "forecast_steps = 23\n",
    "num_edge_types = 4\n",
    "num_nodes = 8\n",
    "\n",
    "# Sample node features\n",
    "x = torch.randn(num_nodes, in_channels)\n",
    "\n",
    "# Sample edges (2 edges per type)\n",
    "edge_index = torch.tensor([\n",
    "    [0, 1, 2, 3, 4, 5, 6, 7],  # Source nodes\n",
    "    [1, 2, 3, 4, 5, 6, 7, 0]   # Target nodes\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Sample edge types\n",
    "edge_type = torch.tensor([0, 0, 1, 1, 2, 2, 3, 3], dtype=torch.long)\n",
    "\n",
    "# Sample target values\n",
    "targets = torch.randn(num_nodes, forecast_steps)\n",
    "\n",
    "# Initialize model\n",
    "model = TimeSeriesGNN(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    forecast_steps=forecast_steps,\n",
    "    num_edge_types=num_edge_types\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "means, log_vars = model(x, edge_index, edge_type)\n",
    "\n",
    "# Calculate loss\n",
    "loss = model.nll_loss(means, log_vars, targets)\n",
    "\n",
    "# Print shapes and values\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Edge index shape: {edge_index.shape}\")\n",
    "print(f\"Predictions shape: {means.shape}\")\n",
    "print(f\"Log variances shape: {log_vars.shape}\")\n",
    "print(f\"Loss value: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
