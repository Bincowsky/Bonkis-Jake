{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Time Series Forecasting with PyTorch\n",
    "\n",
    "This Jupyter Notebook provides a full implementation of training an LSTM model using PyTorch on your pivot table dataset. The steps include data preparation, model definition, training, and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Data Preparation](#1)\n",
    "   - Import Libraries\n",
    "   - Load and Preprocess Data\n",
    "2. [Define Dataset Class](#2)\n",
    "3. [Split Data into Training and Validation Sets](#3)\n",
    "4. [Create DataLoaders](#4)\n",
    "5. [Define the LSTM Model](#5)\n",
    "6. [Initialize Model, Loss Function, and Optimizer](#6)\n",
    "7. [Training Loop](#7)\n",
    "8. [Evaluate the Model](#8)\n",
    "9. [Save the Model](#9)\n",
    "10. [Conclusion](#10)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Data Preparation\n",
    "\n",
    "### Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data\n",
    "\n",
    "Assuming your pivot table dataset in \"../data/processed/2023_complete_pivot.parquet\", with dates as the index and country-brand pairs as columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Aldovia-AIMST</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Aldovia-AIMST</td>\n",
       "      <td>0.006284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>Aldovia-AIMST</td>\n",
       "      <td>0.123459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>Aldovia-AIMST</td>\n",
       "      <td>0.055607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>Aldovia-AIMST</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date             id     value\n",
       "0 2013-01-01  Aldovia-AIMST  0.000000\n",
       "1 2013-01-02  Aldovia-AIMST  0.006284\n",
       "2 2013-01-03  Aldovia-AIMST  0.123459\n",
       "3 2013-01-04  Aldovia-AIMST  0.055607\n",
       "4 2013-01-05  Aldovia-AIMST  0.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df_pivot = pd.read_parquet(\"../data/processed/2023_complete_pivot.parquet\")\n",
    "\n",
    "# Reset index to turn the date into a column\n",
    "df_pivot = df_pivot.reset_index()\n",
    "\n",
    "# Rename the 'index' column to 'date'\n",
    "df_pivot = df_pivot.rename(columns={'index': 'date'})\n",
    "\n",
    "# Melt the DataFrame to long format\n",
    "df = df_pivot.melt(id_vars=['date'], var_name='id', value_name='value')\n",
    "\n",
    "# Sort by 'id' and 'date'\n",
    "df = df.sort_values(['id', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Convert 'date' to datetime if not already\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Define Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, targets, device):\n",
    "        # Convert sequences and targets to torch tensors and move to GPU\n",
    "        self.sequences = torch.FloatTensor(sequences).to(device)  # Shape: [num_samples, sequence_length]\n",
    "        self.targets = torch.FloatTensor(targets).to(device)      # Shape: [num_samples]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve sequences and targets directly from GPU tensors\n",
    "        sequence = self.sequences[idx].unsqueeze(-1)  # Shape: [sequence_length, 1]\n",
    "        target = self.targets[idx]\n",
    "        return sequence, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Split Data into Training and Validation Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 2218278\n",
      "Total validation samples: 514141\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "cutoff_percentage = 0.8\n",
    "sequence_length = 30  # Adjust based on your data\n",
    "ratio_zeros = 0.5\n",
    "\n",
    "# Determine the cutoff date for the split (e.g., last 20% for validation)\n",
    "cutoff_date = df['date'].quantile(cutoff_percentage)\n",
    "cutoff_date = np.datetime64(cutoff_date)\n",
    "\n",
    "# Initialize lists for training and validation data\n",
    "train_sequences = []\n",
    "train_targets = []\n",
    "val_sequences = []\n",
    "val_targets = []\n",
    "\n",
    "def generate_sequences(values, cutoff_idx, seq_length, zero_ratio):\n",
    "    \"\"\"\n",
    "    Generate sequences and targets from the values array.\n",
    "    \n",
    "    Parameters:\n",
    "    - values (np.ndarray): Array of values.\n",
    "    - cutoff_idx (int): Index to split training and validation data.\n",
    "    - seq_length (int): Length of each sequence.\n",
    "    - zero_ratio (float): Maximum allowed ratio of zeros in a sequence.\n",
    "    \n",
    "    Returns:\n",
    "    - train_seq, train_tgt, val_seq, val_tgt: Lists of training and validation sequences and targets.\n",
    "    \"\"\"\n",
    "    num_values = len(values)\n",
    "    \n",
    "    # Training indices\n",
    "    train_end = cutoff_idx - seq_length\n",
    "    train_indices = np.arange(0, train_end)\n",
    "    \n",
    "    # Validation indices\n",
    "    val_start = cutoff_idx\n",
    "    val_end = num_values - seq_length\n",
    "    val_indices = np.arange(val_start, val_end)\n",
    "    \n",
    "    # Function to create sequences and targets\n",
    "    def create_seq_tgt(indices):\n",
    "        seq = np.lib.stride_tricks.sliding_window_view(values, window_shape=seq_length)\n",
    "        tgt = values[seq_length:]\n",
    "        selected_seq = seq[indices]\n",
    "        selected_tgt = tgt[indices]\n",
    "        return selected_seq, selected_tgt\n",
    "    \n",
    "    # Create training sequences and targets\n",
    "    train_seq, train_tgt = create_seq_tgt(train_indices)\n",
    "    \n",
    "    # Create validation sequences and targets\n",
    "    val_seq, val_tgt = create_seq_tgt(val_indices)\n",
    "    \n",
    "    # Calculate ratio of zeros in each sequence\n",
    "    train_zero_ratio = np.mean(train_seq == 0.0, axis=1)\n",
    "    val_zero_ratio = np.mean(val_seq == 0.0, axis=1)\n",
    "    \n",
    "    # Filter sequences based on zero_ratio\n",
    "    train_mask = train_zero_ratio <= zero_ratio\n",
    "    val_mask = val_zero_ratio <= zero_ratio\n",
    "    \n",
    "    # Apply mask\n",
    "    train_seq_filtered = train_seq[train_mask]\n",
    "    train_tgt_filtered = train_tgt[train_mask]\n",
    "    \n",
    "    val_seq_filtered = val_seq[val_mask]\n",
    "    val_tgt_filtered = val_tgt[val_mask]\n",
    "    \n",
    "    return train_seq_filtered, train_tgt_filtered, val_seq_filtered, val_tgt_filtered\n",
    "\n",
    "# Iterate over each group and generate sequences\n",
    "for name, group in df.groupby('id'):\n",
    "    group = group.sort_values('date')\n",
    "    values = group['value'].values\n",
    "    dates = group['date'].values\n",
    "    cutoff_index = np.searchsorted(dates, cutoff_date)\n",
    "    \n",
    "    # Ensure cutoff_index is within bounds\n",
    "    cutoff_index = min(cutoff_index, len(values))\n",
    "    \n",
    "    # Generate sequences and targets\n",
    "    train_seq, train_tgt, val_seq, val_tgt = generate_sequences(\n",
    "        values, cutoff_index, sequence_length, ratio_zeros\n",
    "    )\n",
    "    \n",
    "    # Append to the main lists\n",
    "    train_sequences.append(train_seq)\n",
    "    train_targets.append(train_tgt)\n",
    "    val_sequences.append(val_seq)\n",
    "    val_targets.append(val_tgt)\n",
    "\n",
    "# Concatenate all sequences and targets\n",
    "train_sequences = np.concatenate(train_sequences) if train_sequences else np.array([])\n",
    "train_targets = np.concatenate(train_targets) if train_targets else np.array([])\n",
    "val_sequences = np.concatenate(val_sequences) if val_sequences else np.array([])\n",
    "val_targets = np.concatenate(val_targets) if val_targets else np.array([])\n",
    "\n",
    "print(f'Total training samples: {len(train_sequences)}')\n",
    "print(f'Total validation samples: {len(val_sequences)}')\n",
    "\n",
    "# Convert to lists of sequences and targets for Dataset\n",
    "train_sequences = list(train_sequences)\n",
    "train_targets = list(train_targets)\n",
    "val_sequences = list(val_sequences)\n",
    "val_targets = list(val_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Create DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_20064\\3986715460.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  self.sequences = torch.FloatTensor(sequences).to(device)  # Shape: [num_samples, sequence_length]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training batches: 34\n",
      "Total validation batches: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "batch_size = 2**16 # 64000\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create dataset instances with device parameter\n",
    "train_dataset = TimeSeriesDataset(train_sequences, train_targets, device)\n",
    "val_dataset = TimeSeriesDataset(val_sequences, val_targets, device)\n",
    "\n",
    "# Optimize DataLoader parameters\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,  # Use almost full batch\n",
    "    shuffle=True,\n",
    "    num_workers=0,                 # Set to 0 to avoid multiprocessing issues with GPU tensors\n",
    "    pin_memory=False               # Disable pin_memory as data is already on GPU\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,    # Use almost full batch\n",
    "    shuffle=False,\n",
    "    num_workers=0,                 # Set to 0 for the same reason as above\n",
    "    pin_memory=False               # Disable pin_memory\n",
    ")\n",
    "\n",
    "print(f'Total training batches: {len(train_loader)}')\n",
    "print(f'Total validation batches: {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Define the LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForecastingModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2):\n",
    "        super(LSTMForecastingModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Define output layer\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Get the output from the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. Initialize Model, Loss Function, and Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LSTMForecastingModel(hidden_size=8).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## 7. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.072314, Val Loss: 0.029642, Elapsed time: 59.62 seconds\n",
      "Epoch [2/20], Train Loss: 0.010497, Val Loss: 0.001920, Elapsed time: 58.50 seconds\n",
      "Epoch [3/20], Train Loss: 0.001964, Val Loss: 0.001920, Elapsed time: 58.33 seconds\n",
      "Epoch [4/20], Train Loss: 0.001873, Val Loss: 0.001920, Elapsed time: 58.23 seconds\n",
      "Epoch [5/20], Train Loss: 0.001871, Val Loss: 0.001920, Elapsed time: 58.96 seconds\n",
      "Epoch [6/20], Train Loss: 0.001870, Val Loss: 0.001919, Elapsed time: 60.95 seconds\n",
      "Epoch [7/20], Train Loss: 0.001870, Val Loss: 0.001919, Elapsed time: 58.29 seconds\n",
      "Epoch [8/20], Train Loss: 0.001870, Val Loss: 0.001919, Elapsed time: 58.62 seconds\n",
      "Epoch [9/20], Train Loss: 0.001870, Val Loss: 0.001919, Elapsed time: 58.79 seconds\n",
      "Epoch [10/20], Train Loss: 0.001870, Val Loss: 0.001919, Elapsed time: 60.14 seconds\n",
      "Epoch [11/20], Train Loss: 0.001870, Val Loss: 0.001919, Elapsed time: 58.95 seconds\n",
      "Epoch [12/20], Train Loss: 0.001870, Val Loss: 0.001919, Elapsed time: 58.76 seconds\n",
      "Epoch [13/20], Train Loss: 0.001870, Val Loss: 0.001919, Elapsed time: 58.65 seconds\n",
      "Epoch [14/20], Train Loss: 0.001870, Val Loss: 0.001919, Elapsed time: 58.94 seconds\n",
      "Epoch [15/20], Train Loss: 0.001869, Val Loss: 0.001918, Elapsed time: 59.03 seconds\n",
      "Epoch [16/20], Train Loss: 0.001869, Val Loss: 0.001918, Elapsed time: 58.91 seconds\n",
      "Epoch [17/20], Train Loss: 0.001869, Val Loss: 0.001918, Elapsed time: 59.69 seconds\n",
      "Epoch [18/20], Train Loss: 0.001869, Val Loss: 0.001918, Elapsed time: 58.92 seconds\n",
      "Epoch [19/20], Train Loss: 0.001869, Val Loss: 0.001918, Elapsed time: 58.70 seconds\n",
      "Epoch [20/20], Train Loss: 0.001869, Val Loss: 0.001918, Elapsed time: 59.39 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 20  # Adjust based on your needs\n",
    "last_epoch_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for sequences, targets in train_loader:\n",
    "        # No need to move data to device as it's already on GPU\n",
    "        # sequences and targets are GPU tensors\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences)  # Shape: [batch_size, 1]\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in val_loader:\n",
    "            # No need to move data to device\n",
    "            outputs = model(sequences).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_losses.append(loss.item())\n",
    "    \n",
    "    elapsed_time = time.time() - last_epoch_time\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Train Loss: {np.mean(train_losses):.6f}, '\n",
    "          f'Val Loss: {np.mean(val_losses):.6f}, '\n",
    "          f'Elapsed time: {elapsed_time:.2f} seconds')\n",
    "    last_epoch_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "## 8. Evaluate the Model\n",
    "\n",
    "### Calculate Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 0.0322\n",
      "Validation RMSE: 0.0438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\Bonkis-Jake\\venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for sequences, targets in val_loader:\n",
    "        outputs = model(sequences)\n",
    "        predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "        actuals.extend(targets.cpu().numpy())\n",
    "        i += 1\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "rmse = mean_squared_error(actuals, predictions, squared=False)\n",
    "\n",
    "print(f'Validation MAE: {mae:.4f}')\n",
    "print(f'Validation RMSE: {rmse:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 100 predictions vs actuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actuals[:100], label='Actual')\n",
    "plt.plot(predictions[:100], label='Predicted')\n",
    "plt.legend()\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = np.array(actuals[:100]) - np.array(predictions[:100])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual vs Predicted\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(actuals[:100], label='Actual', color='blue')\n",
    "plt.plot(predictions[:100], label='Predicted', color='red')\n",
    "plt.legend()\n",
    "plt.title('Actual vs Predicted Values')\n",
    "\n",
    "# Residuals\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(residuals, label='Residuals', color='green')\n",
    "plt.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('Residuals')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Actual - Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "## 9. Save the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'lstm_forecasting_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a>\n",
    "## 10. Conclusion\n",
    "\n",
    "You've successfully trained an LSTM model on your pivot table dataset using PyTorch. You can now use this model to make predictions on future data or further refine it by tuning hyperparameters or incorporating additional features.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Notes\n",
    "\n",
    "- **Adjust Hyperparameters**: Experiment with `sequence_length`, `hidden_size`, `num_layers`, `batch_size`, and `num_epochs` to optimize performance.\n",
    "- **Scaling Data**: If your data has varying scales, consider scaling or normalizing it.\n",
    "- **Early Stopping**: Implement early stopping to prevent overfitting if necessary.\n",
    "- **Feature Engineering**: Even without exogenous variables, adding features like day of the week or month can help capture temporal patterns.\n",
    "\n",
    "## Example: Making Future Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(model, initial_sequence, prediction_length):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    sequence = initial_sequence.copy()\n",
    "    \n",
    "    for _ in range(prediction_length):\n",
    "        seq_input = torch.FloatTensor(sequence[-sequence_length:]).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(seq_input)\n",
    "        pred_value = pred.item()\n",
    "        predictions.append(pred_value)\n",
    "        sequence = np.append(sequence, pred_value)\n",
    "    return predictions\n",
    "\n",
    "# Example usage:\n",
    "# Get the last sequence from the validation data\n",
    "latest_sequence = val_sequences[-1]\n",
    "future_predictions = predict_future(model, latest_sequence, prediction_length=7)\n",
    "\n",
    "print(\"Future Predictions:\", future_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
