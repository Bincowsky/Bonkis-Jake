{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/raw/train_data.csv')\n",
    "train_df.replace('-1', np.nan, inplace=True)\n",
    "train_df.replace(np.float64(-1.0), np.nan, inplace=True)\n",
    "submission_df = pd.read_csv('../data/raw/submission_data.csv')\n",
    "submission_df.replace('-1', np.nan, inplace=True)\n",
    "submission_df.replace(np.float64(-1.0), np.nan, inplace=True)\n",
    "template_df = pd.read_csv('../data/processed/submission_template.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = [\"launch_date\", \"date\"]  # Adjust as necessary\n",
    "for col in date_cols:\n",
    "    train_df[col] = pd.to_datetime(train_df[col], errors='coerce')\n",
    "    train_df[f\"{col}_year\"] =  train_df[col].dt.year\n",
    "    train_df[f\"{col}_month\"] = train_df[col].dt.month\n",
    "    train_df[f\"{col}_day\"] =   train_df[col].dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaig a implementar que el test set siguin els top 20% últims llençaments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_train_samples = 0.8\n",
    "\n",
    "launches = train_df.groupby('cluster_nl')['launch_date'].first().reset_index()\n",
    "launches = launches.sort_values('launch_date')\n",
    "print(launches)\n",
    "cutoff = int(len(launches) * perc_train_samples)\n",
    "cutoff_launch_date = launches.iloc[cutoff]['launch_date']\n",
    "print(f\"Train cutoff: {cutoff_launch_date}\")\n",
    "train_cluster_nls = launches.iloc[:cutoff]['cluster_nl']\n",
    "test_cluster_nls = launches.iloc[cutoff:]['cluster_nl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"All data shape: {train_df.shape}\")\n",
    "train_data = train_df.loc[train_df['cluster_nl'].isin(train_cluster_nls)]\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "test_data = train_df.loc[train_df['cluster_nl'].isin(test_cluster_nls)]\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Separate features and target for training and testing sets\n",
    "X_train = train_data.drop(columns=['target'])\n",
    "y_train = train_data['target']\n",
    "\n",
    "X_test = test_data.drop(columns=['target'])\n",
    "y_test = test_data['target']\n",
    "\n",
    "# keep this convenient model to analyze the actual performance metric\n",
    "metric_df = test_data[['cluster_nl', 'date', 'target']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X_train.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", xgb_model)\n",
    "])\n",
    "\n",
    "# 5. Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    \"regressor__n_estimators\": [300],\n",
    "    \"regressor__learning_rate\": [0.03, 0.1, 0.3],\n",
    "    \"regressor__max_depth\": [7],\n",
    "    \"regressor__colsample_bytree\": [0.7, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring=\"neg_mean_squared_error\", verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search.cv_results_\n",
    "mean_losses = results['mean_test_score']\n",
    "results_df = pd.DataFrame(results)\n",
    "# Create a pivot table with row = learning rate, col = regressor__colsample_bytree\n",
    "pivot_table = results_df.pivot_table(index='param_regressor__learning_rate', columns='param_regressor__colsample_bytree', values='rank_test_score')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import compute_metric, _metrics, unaveraged_CYME\n",
    "def predict_and_measure_performance(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"MSE:  {mse}\")\n",
    "    \n",
    "    cyme = unaveraged_CYME(metric_df, y_pred)\n",
    "    print(f\"CYME: {cyme}\")\n",
    "    return mse, cyme\n",
    "\n",
    "mse, cyme = predict_and_measure_performance(best_model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "\n",
    "\n",
    "# Extract the XGBRegressor from the pipeline\n",
    "xgb_regressor = best_model.named_steps[\"regressor\"]\n",
    "\n",
    "# Handle feature names based on the preprocessor\n",
    "preprocessor = best_model.named_steps[\"preprocessor\"]\n",
    "\n",
    "# Get transformed feature names\n",
    "if hasattr(preprocessor, \"get_feature_names_out\"):\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "else:\n",
    "    # If the preprocessor does not support this, use generic feature indices\n",
    "    feature_names = [f\"Feature_{i}\" for i in range(xgb_regressor.feature_importances_.shape[0])]\n",
    "\n",
    "# Get feature importances\n",
    "importance = xgb_regressor.feature_importances_\n",
    "\n",
    "# Create a DataFrame for sorting and visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names[:50],\n",
    "    \"Importance\": importance[:50]\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Enhanced bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Importance\", fontsize=14)\n",
    "plt.ylabel(\"Features\", fontsize=8)\n",
    "plt.title(\"Feature Importance (Sorted)\", fontsize=16)\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for highest importance at the top\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = importance_df[importance_df.Feature.str.contains('num')]\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(df_num[\"Feature\"], df_num[\"Importance\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Importance\", fontsize=14)\n",
    "plt.ylabel(\"Features\", fontsize=8)\n",
    "plt.title(\"Feature Importance (Sorted)\", fontsize=16)\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for highest importance at the top\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = submission_df.drop(columns=['target'])\n",
    "y = submission_df['target']\n",
    "date_cols = [\"launch_date\", \"date\"]  # Adjust as necessary\n",
    "for col in date_cols:\n",
    "    features_test[col] = pd.to_datetime(features_test[col], errors='coerce')\n",
    "    features_test[f\"{col}_year\"] = features_test[col].dt.year\n",
    "    features_test[f\"{col}_month\"] = features_test[col].dt.month\n",
    "    features_test[f\"{col}_day\"] = features_test[col].dt.day\n",
    "features_test.drop(columns=date_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cols = set(X_train.columns) - set(features_test.columns)\n",
    "for col in missing_cols:\n",
    "    features_test[col] = 0 \n",
    "\n",
    "new_data = features_test[X_train.columns]\n",
    "predictions = best_model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_with_predictions = new_data.copy()\n",
    "new_data_with_predictions[\"prediction\"] = predictions\n",
    "date_cols = [\"date\"]\n",
    "for col in date_cols:\n",
    "    year_col = f\"{col}_year\"\n",
    "    month_col = f\"{col}_month\"\n",
    "    day_col = f\"{col}_day\"\n",
    "    \n",
    "    if all(c in new_data_with_predictions.columns for c in [year_col, month_col, day_col]):\n",
    "        new_data_with_predictions[col] = pd.to_datetime(\n",
    "            dict(year=new_data_with_predictions[year_col], \n",
    "                 month=new_data_with_predictions[month_col], \n",
    "                 day=new_data_with_predictions[day_col]),\n",
    "            errors='coerce'\n",
    "        )\n",
    "        # Drop the individual year, month, and day columns if necessary\n",
    "        new_data_with_predictions.drop(columns=[year_col, month_col, day_col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = new_data_with_predictions.drop(columns=['brand', 'che_pc_usd', 'che_perc_gdp', 'corporation',\n",
    "       'country', 'drug_id', 'ind_launch_date', 'indication',\n",
    "       'insurance_perc_che', 'population', 'prev_perc', 'price_month',\n",
    "       'price_unit', 'public_perc_che', 'therapeutic_area', 'launch_date_year',\n",
    "       'launch_date_month', 'launch_date_day', 'launch_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template_df[\"date\"] = pd.to_datetime(template_df[\"date\"], errors=\"coerce\")\n",
    "final_df[\"date\"] = pd.to_datetime(final_df[\"date\"], errors=\"coerce\")\n",
    "filled_df = template_df.merge(\n",
    "    final_df, \n",
    "    on=[\"date\", \"cluster_nl\"], \n",
    "    how=\"left\", \n",
    "    suffixes=(\"\", \"_pred\")\n",
    ")\n",
    "filled_df[\"prediction\"] = filled_df[\"prediction\"].fillna(filled_df[\"prediction_pred\"])\n",
    "filled_df.drop(columns=[\"prediction_pred\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_df.to_csv('../data/outputs/try1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
